{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/04/03/hello-world/"},{"title":"人类世界思维九大模型","text":"在张首晟看来，人类历史上有9个最伟大的思维模型。我想以一个问题开始：假设世界末日到了，诺亚方舟上只能够携带一对动物和一个信封，信封背面，你可以总结所有人类知识，那你们会写下什么？我分享下我会写下什么，这堂课将围绕这些答案： 九大模型自然界三大基本常数 作为一个物理学家，我觉得人类文明最高的建树还是科学真理。科学真理最重要的是两点，一是能量，二是信息。对能量最深刻的认识来自爱因斯坦，如果总结人类最知名的公式，E=mc²是首选。大家说物质本身也很重要，但是通过E=mc²，就发现物质和能量是一回事。对信息的认识，是人类对大自然最重要的认识。怎么描写和衡量信息，也有一个非常奇妙信息熵的公式：S=-p log p。这个公式不像E=mc²那么知名，但爱因斯坦说过一句话：等到人类的知识往前推进，牛顿力学可能不对，量子力学可能不对，相对论可能也不对，但信息熵的公式是永恒的。我们这个时代，可能觉得科学万能，但第三个公式告诉我们，科学不是万能的。科学的伟大，在于能够告诉科学的界限在哪儿。这就是量子力学的海森堡测不准原理，科学有一个永远不能跨过的界限，你不可能把一个粒子的位置，和它的重量，或者它的速度，同时精准地测量出来。哲学上讲，客观世界完全独立于主观世界的存在，但是这是个伪命题，并不是真正存在的。真正能观察到的，是客观和主观之间的结合，由于观察者和被观察的世界相互作用，我们不可能无穷精准地把客观世界了解清楚。这是第三个公式的伟大，因为它不只是讲了一个科学的原理，更说明了一个哲学的原理，告诉我们客观的世界和主观的世界，不能完全割裂。 万物都是由原子构成人类的文明不只局限于科学，还有哲学，政治，法律⋯⋯如果用一句话来描写整个世界里，最值得告诉下一代的话，就是“万物都是由原子构成”。譬如中医讲“气”，好象是非常神奇的东西，这个气，本身也是由原子组成的。因为世界上不可能有别的物质产生。不只是地球，整个宇宙的原子，和我们这里的原子完全是一模一样的。我们怎么会知道？因为每个原子会发出一些特殊的光来。如果说我们知道太阳上有氦原子，氦原子发出光的那个性质，所谓的光谱和地球上能够观察到的光谱一模一样。整个宇宙世界里，万物都是由原子构成的。这个概念是个非常伟大的概念，虽然真正对原子的理解，是20世纪初由于量子力学的奠基。但是了不起的是两千年以前，原子的概念就被希腊的人提出来了。所以，这是一种思想方法，就是要把世界上万物的复杂性归纳到原子的简单性。我们知道元素周期表特别简单，那么千变万化的世界，归纳到最后就是这100多种元素，这就是思想方法的伟大。好像是非常抽象的科学概念，和我们今天办企业有什么关系？做任何的企业要能赚钱，就是用简单的办法来解决一个复杂的问题。 欧几里德几何定理以上是物理的概念，也是化学的概念，如果用数学来描写，最精华的思想是什么？希腊文明的伟大，除了那些公式之外，是把数学思想已经基本奠定了，这就是欧几里得的公理系统。这么错综复杂的理论体系，它一定要建设在五条不言而喻的公理上面：比如两点可以划一条直线；所有直角都全等⋯⋯这个事情我们觉得真是太不言而喻了，根本不需要证明。欧几里得把它作为基本的公理，就是第一性原理。再复杂的知识体系，你一定要归纳成最少的几条不言而喻的公理。欧几里得的妙处就是出发于那些显而易见的公理，来整理他整个知识的体系，把它变成一个丰茂的大树，这些大树的根就是一些公理。在这种情况下，物理的思想和几何的思想也是统一的思想，把万物归到一个最最简单的原理上。 自然选择，适者生存生物是一个非常复杂的系统，好像不容易总结出非常简单而普世的规律，其实，如果整个生物用一句话来描写的话，就是“自然选择，适者生存”。 所以，生物之所以能够进化，是因为周围的环境一直在改变，能够适应环境的生物就能生存，不适应的就会被淘汰，这是进化的最原始动力。我们每次讲公司的生和死，其实已经潜意识把公司看成是生物，公司也是同样适者生存，竞争是非常残酷的，但是越是极地环境下生存的公司，就越优秀。 人人生来平等讲起人文的思想，如果用一句话来总结的话，就是“人人生来平等”。这是美国《独立宣言》上的第一句话，基本上模仿了欧几里得的公理系统。它说我要建一个国家，国家是一个非常复杂的体系，但也要建在一个基本的原理上。这个基本的原理，一定要是每个公民觉得是不言而喻的一个真理，不需要过多的解释。美国的宪法200年不变，为什么？它是把科学的基本理念用在治国。既然欧几里得复杂的定理系统能够建立在几条显而易见的公理上，我们建国为什么不可以？使得每个公民根本不需要任何知识，一讲他就知道？这样国家才能真正地牢固，这是人人生来平等，这是一个基本的原理。 自由的空气在飘扬说到办一流的大学，大家都会想到哈佛大学和斯坦福大学这些百年学府。斯坦福大学牛在哪儿？它的校训是“自由的空气在飘扬”。这是一个百年学府成功的前沿思想，就是学术环境一定要有自由，一不要受政治的影响，二不要受功利心的影响。 笔胜于剑如果我要总结整个人类文明几千年的历史，用一句话来描写，我会说“笔胜于剑”，笔远远比剑来得更重要。历史上，亚历山大征服了世界。他有两位老师，一位是他的父亲腓力二世，教他怎么用武力征服已知的世界。另一位老师叫亚里士多德，同样教他改变世界，不是用武力，而是知识，真正把人类的知识彻底整合，彻底征服。所以亚历山大有两个改变世界的志向：一是征服人类已知的世界，他打到阿富汗时痛苦一场，认为下面没有可以征服的地方；二是亚历山大图书馆，他的目标是收集人类所有的书籍。他征服过来的帝国，在他过世一年之内就全部崩溃，但他真正给人类文明留下的是亚历山大图书馆，由于信息极端密集，也吸引了很多学者汇聚，影响了人类历史。这个道理很简单，但大家在用的时候，往往不知道它的深意，希望大家能通过这些故事深入思考。 隐形的手如果讲起经济学，也要有一句话，我想就是亚当斯密所讲的，“市场是一个隐形的手”。这句话的道理，我想大家已经讲得很多，但是做起来，往往会不太明白，尤其是政府在做事情的时候，往往就有一个无形的手就会变成一个有形的手。但是这个道理非常的深刻，你在用有形的手干预市场的话，很短时间内可能会有些效果，但肯定不能打造百年老店，不可能形成千年思想。 大道至简刚才这些都是西方人留下的思想，我们中国千年的文明，留下的思想哪一句应该写到信封背面呢？“大道至简”。很多人说起谁牛，就说是专家，其实专家并不牛，把大道用简单的话讲出来，让人人都听懂，这才是真正牛的。千年思想的确最最精华的都是大道至简，你看宇宙美妙在哪儿？E=MC²这样一个公式，能够描写小到原子，大到宇宙。真理的共通点就是“大道至简”。讲个故事，4的根号等于几？很简单，2和-2，英国理论物理学家、量子力学的奠基者之一狄拉克初中时，就觉得这个回答非常非常奇妙，为什么开根号的时候总是有一个正根，有一个负根？他突然想到把这个原理推广了一下，就说宇宙上面所有的基本粒子，都有个反粒子，有个电子就有个反电子，有个质子就有个反质子，有个中子就有个反中子，这是个非常非常神奇的发现。所以，科学大师，你看他为什么能够做出这些伟大的科学发现？就是他始终没有忘记“大道至简”，后来狄拉克得到了诺贝尔奖。 引用于: 张首晟推荐的人类最伟大的9大思维模型","link":"/2020/04/12/%E4%BA%BA%E7%B1%BB%E4%B8%96%E7%95%8C%E6%80%9D%E7%BB%B4%E4%B9%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"title":"信息熵相关概念","text":"机器学习中，绕不开的一个概念就是熵 (Entropy)，信息熵。信息熵常被用来作为一个系统的信息含量的量化指标，从而可以进一步用来作为系统方程优化的目标或者参数选择的判据。在决策树的生成过程中，就使用了熵来作为样本最优属性划分的判据。下面按照本人的理解来系统梳理一下有关熵的概念。 熵的定义是怎么来的？信息熵的定义公式： 并且规定 0log(0)=0。 首次看到这个定义，我感到莫名其妙，为什么熵要定义成这么复杂的形式，并且其中还出现了对数函数这种非常不直观的表述？ 信息熵的三个性质信息论之父克劳德·香农给出的信息熵的三个性质[1]： 单调性，发生概率越高的事件，其携带的信息量越低；非负性，信息熵可以看作为一种广度量，非负性是一种合理的必然；累加性，即多随机事件同时发生存在的总不确定性的量度是可以表示为各事件不确定性的量度的和，这也是广度量的一种体现。香农从数学上严格证明了满足上述三个条件的随机变量不确定性度量函数具有唯一形式 其中的 C 为常数，我们将其归一化为 C=1 即得到了信息熵公式。 对信息熵三条性质的理解单调性说的是，事件发生的概率越低，其发生时所能给出的信息量越大。举一个极端的例子，“太阳从西边升起”所携带的信息量就远大于“太阳从东边升起”，因为后者是一个万年不变的事实，不用特意述说大家都知道；而前者是一个相当不可能发生的事情，如果发生了，那代表了太多的可能性，可能太阳系有重大变故，可能物理法则发生了变化，等等。从某种角度来考虑，单调性也暗含了一种对信息含量的先验假设，即默认某些事实是不含信息量的（默认事实其实也是一种信息，我理解的默认事实应该指的是概率分布），这其实是把默认情况的信息量定标为 0 了 引用于: 信息熵及其相关概念","link":"/2020/04/12/%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/"},{"title":"mnist识别神经网络可视化","text":"MNIST数据集是机器学习领域中非常经典的一个数据集，由60000个训练样本和10000个测试样本组成，每个样本都是一张28 * 28像素的灰度手写数字图片。 下载官方网站 http://yann.lecun.com/exdb/mnist/一共4个文件，训练集、训练集标签、测试集、测试集标签 文件名称大小内容 train-images-idx3-ubyte.gz9,681 kb55000张训练集，5000张验证集 train-labels-idx1-ubyte.gz29 kb训练集图片对应的标签 t10k-images-idx3-ubyte.gz1,611 kb10000张测试集 t10k-labels-idx1-ubyte.gz5 kb测试集图片对应的标签 读入数据集直接下载下来的数据是无法通过解压或者应用程序打开的，因为这些文件不是任何标准的图像格式而是以字节的形式进行存储的，所以必须编写程序来打开它。 使用TensorFlow进行解压使用TensorFlow中input_data.py脚本来读取数据及标签，使用这种方式时，可以不用事先下载好数据集，它会自动下载并存放到你指定的位置。 123456789101112131415161718192021222324252627282930from tensorflow.examples.tutorials.mnist import input_dataimport matplotlib.pyplot as pltmnist = input_data.read_data_sets('MNIST_data',one_hot=True) # MNIST_data指的是存放数据的文件夹路径，one_hot=True 为采用one_hot的编码方式编码标签#load datatrain_X = mnist.train.images #训练集样本validation_X = mnist.validation.images #验证集样本test_X = mnist.test.images #测试集样本#labelstrain_Y = mnist.train.labels #训练集标签validation_Y = mnist.validation.labels #验证集标签test_Y = mnist.test.labels #测试集标签print(train_X.shape,train_Y.shape) #输出训练集样本和标签的大小#查看数据，例如训练集中第一个样本的内容和标签print(train_X[0]) #是一个包含784个元素且值在[0,1]之间的向量print(train_Y[0])#可视化样本，下面是输出了训练集中前20个样本fig, ax = plt.subplots(nrows=4,ncols=5,sharex='all',sharey='all')ax = ax.flatten()for i in range(20): img = train_X[i].reshape(28, 28) ax[i].imshow(img,cmap='Greys')ax[0].set_xticks([])ax[0].set_yticks([])plt.tight_layout()plt.show() 123(55000, 784) (55000, 10) #训练集样本和标签的大小第一个样本的内容输出较多，省略[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] #第一个样本的标签，one-hot编码，只有对应位置的值是1，其余都是0 mnist数据集神经网络训练: 神经网络可视化: 涉及材料","link":"/2020/04/12/mnist%E8%AF%86%E5%88%AB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"理解lstm神经网络","text":"（译）理解 LSTM 网络 （Understanding LSTM Networks by colah）Understanding LSTM Networks 循环神经网络（RNNs)人们思考问题往往不是从零开始的。就好像你现在阅读这篇文章一样，你对每个词的理解都会依赖于你前面看到的一些词，而不是把你前面看的内容全部抛弃了，忘记了，再去理解这个单词。也就是说，人们的思维总是会有延续性的。 传统的神经网络是做不到这样的延续性（它们没办法保留对前文的理解），这似乎成了它们一个巨大的缺陷。举个例子，在观看影片中，你想办法去对每一帧画面上正在发生的事情做一个分类理解。目前还没有明确的办法利用传统的网络把对影片中前面发生的事件添加进来帮助理解后面的画面。 但是，循环神经网络可以做到。在RNNs的网络中，有一个循环的操作，使得它们能够保留之前学习到的内容。 在上图网络结构中，对于矩形块 A 的那部分，通过输入$ x_t （t时刻的特征向量），它会输出一个结果（t时刻的特征向量），它会输出一个结果 h_t $（t时刻的状态或者输出）。网络中的循环结构使得某个时刻的状态能够传到下一个时刻。（译者注：因为当前时刻的状态会作为下一时刻输入的一部分） 这些循环的结构让 RNNs 看起来有些难以理解。但是，你稍微想一下就会发现，这似乎和普通的神经网络有不少相似之处呀。我们可以把 RNNs 看成是一个普通的网络做了多次复制后叠加在一起组成的。每一网络会把它的输出传递到下一个网络中。我们可以把 RNNs 在时间步上进行展开，就得到下图这样： 从 RNNs 链状的结构很容易理解到它是和序列信息相关的。这种结构似乎生来就是为了解决序列相关问题的。 而且，它们的的确确非常管用！在最近的几年中，人们利用 RNNs 不可思议地解决了各种各样的问题：语音识别，语言模型，翻译，图像（添加）字幕，等等。关于RNNs在这些方面取得的惊人成功，我们可以看 Andrej Karpathy 的博客： The Unreasonable Effectiveness of Recurrent Neural Networks. RNNs 能够取得这样的成功，主要还是 LSTMs 的使用。这是一种比较特殊的 RNNs，而且对于很多任务，它比普通的 RNNs 效果要好很多很多！基本上现在所使用的循环神经网络用的都是 LSTMs，这也正是本文后面所要解释的网络。 长时期依赖存在的问题RNNs 的出现，主要是因为它们能够把以前的信息联系到现在，从而解决现在的问题。比如，利用前面的画面，能够帮助我们理解当前画面的内容。如果 RNNs 真的可以做到这个，那么它肯定是对我们的任务有帮助的。但是它真的可以 做到吗，恐怕还得看实际情况呀！ 有时候，我们在处理当前任务的时候，只需要看一下比较近的一些信息。比如在一个语言模型中，我们要通过上文来预测一下个词可能会是什么，那么当我们看到“ the clouds are in the ?”时，不需要更多的信息，我们就能够自然而然的想到下一个词应该是“sky”。在这样的情况下，我们所要预测的内容和相关信息之间的间隔很小，这种情况下 RNNs 就能够利用过去的信息， 很容易的实现。 但是，有些情况是需要更多的上下文信息。比如我们要预测“I grew up in France … (此处省略1万字)… I speak ?”这个预测的词应该是 Franch，但是我们是要通过很长很长之前提到的信息，才能做出这个正确的预测的呀，普通的 RNNs 很难做到这个。 随着预测信息和相关信息间的间隔增大， RNNs 很难去把它们关联起来了。 从理论上来讲，通过选择合适的参数，RNNs 确实是可以把这种长时期的依赖关系（“long-term dependencies”） 联系起来，并解决这类问题的。但遗憾的是在实际中， RNNs 无法解决这个问题。 Hochreiter (1991) [German] 和 Bengio, et al. (1994) 曾经对这个问题进行过深入的研究，发现 RNNs 的确很难解决这个问题。 但是非常幸运，LSTMs 能够帮我们解决这个问题。 LSTM 网络长短期记忆网络（Long Short Term Memory networks） - 通常叫做 “LSTMs” —— 是 RNN 中一个特殊的类型。由Hochreiter &amp; Schmidhuber (1997)提出，广受欢迎，之后也得到了很多人们的改进调整。LSTMs 被广泛地用于解决各类问题，并都取得了非常棒的效果。 明确来说，设计 LSTMs 主要是为了避免前面提到的 长时期依赖 （long-term dependency ）的问题。它们的本质就是能够记住很长时期内的信息，而且非常轻松就能做到。 所有循环神经网络结构都是由完全相同结构的（神经网络）模块进行复制而成的。在普通的RNNs 中，这个模块结构非常简单，比如仅是一个单一的 tanh 层。 LSTMs 也有类似的结构（译者注：唯一的区别就是中间部分）。但是它们不再只是用一个单一的 tanh 层，而是用了四个相互作用的层。 别担心，别让这个结构给吓着了，下面根据这个结构，我们把它解剖开，一步一步地来理解它（耐心看下去，你一定可以理解的）。现在，我们先来定义一下用到的符号： 在网络结构图中，每条线都传递着一个向量，从一个节点中输出，然后输入到另一个节点中。粉红色的圆圈表示逐点操作，比如向量相加；黄色的矩形框表示的是一个神经网络层（就是很多个神经节点）；合并的线表示把两条线上所携带的向量进行合并（比如一个带 ht−1,另一个带 xt, 那么合并后的输出就是$[h_{t-1}, x_t] $）; 分开的线表示将线上传递的向量复制一份，传给两个地方。 LSTMs 的核心思想LSTMs 最关键的地方在于 cell（整个绿色的框就是一个 cell） 的状态 和 结构图上面的那条横穿的水平线。 cell 状态的传输就像一条传送带，向量从整个 cell 中穿过，只是做了少量的线性操作。这种结构能够很轻松地实现信息从整个 cell 中穿过而不做改变。（译者注：这样我们就可以实现了长时期的记忆保留了） 若只有上面的那条水平线是没办法实现添加或者删除信息的。而是通过一种叫做 门（gates） 的结构来实现的。 门 可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层 和一个逐点相乘的操作来实现的。 sigmoid 层输出（是一个向量）的每个元素都是一个在 0 和 1 之间的实数，表示让对应信息通过的权重（或者占比）。比如， 0 表示“不让任何信息通过”， 1 表示“让所有信息通过”。 每个 LSTM 有三个这样的门结构，来实现保护和控制信息。（译者注：分别是 “forget gate layer”, 遗忘门； “input gate layer”，传入门； “output gate layer”, 输出门） 逐步理解 LSTM（好了，终于来到最激动的时刻了） 遗忘门首先是 LSTM 要决定让那些信息继续通过这个 cell，这是通过一个叫做“forget gate layer ”的sigmoid 神经层来实现的。它的输入是$ h_{t-1} 和和 x_t $，输出是一个数值都在 0，1 之间的向量（向量长度和 cell 的状态 $ C_{t-1} $ 一样），表示让 $C_{t-1} $ 的各部分信息通过的比重。 0 表示“不让任何信息通过”， 1 表示“让所有信息通过”。 回到我们上面提到的语言模型中，我们要根据所有的上文信息来预测下一个词。这种情况下，每个 cell 的状态中都应该包含了当前主语的性别信息（保留信息），这样接下来我们才能够正确地使用代词。但是当我们又开始描述一个新的主语时，就应该把上文中的主语性别给忘了才对(忘记信息)。 传入门下一步是决定让多少新的信息加入到 cell 状态 中来。实现这个需要包括两个 步骤：首先，一个叫做“input gate layer ”的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容，Ct˜Ct。在下一步，我们把这两部分联合起来，对 cell 的状态进行一个更新。 在我们的语言模型的例子中，我们想把新的主语性别信息添加到 cell 状态中，来替换掉老的状态信息。有了上述的结构，我们就能够更新 cell 状态了， 即把$ C_{t-1} $更新为 $C_{t} $。 从结构图中应该能一目了然， 首先我们把旧的状态 C_{t-1} 和和 f_t 相乘，把一些不想保留的信息忘掉。然后加上相乘，把一些不想保留的信息忘掉。然后加上 i_t * \\tilde{C_{t}} 。这部分信息就是我们要添加的新内容。 输出门最后，我们需要来决定输出什么值了。这个输出主要是依赖于 cell 的状态$ C_t$，但是又不仅仅依赖于 C_t ，而是需要经过一个过滤的处理。首先，我们还是使用一个sigmoid层来（计算出）决定，而是需要经过一个过滤的处理。首先，我们还是使用一个sigmoid层来（计算出）决定 C_t 中的哪部分信息会被输出。接着，我们把中的哪部分信息会被输出。接着，我们把 C_t 通过一个 tanh 层（把数值都归到 -1 和 1 之间），然后把 tanh 层的输出和 sigmoid 层计算出来的权重相乘，这样就得到了最后输出的结果。 在语言模型例子中，假设我们的模型刚刚接触了一个代词，接下来可能要输出一个动词，这个输出可能就和代词的信息相关了。比如说，这个动词应该采用单数形式还是复数的形式，那么我们就得把刚学到的和代词相关的信息都加入到 cell 状态中来，才能够进行正确的预测。 LSTM 的变种 GRU原文这部分介绍了 LSTM 的几个变种，还有这些变形的作用。在这里我就不再写了。有兴趣的可以直接阅读原文。 下面主要讲一下其中比较著名的变种 GRU（Gated Recurrent Unit ），这是由 Cho, et al. (2014) 提出。在 GRU 中，如 fig.13 所示，只有两个门：重置门（reset gate）和更新门（update gate）。同时在这个结构中，把细胞状态和隐藏状态进行了合并。最后模型比标准的 LSTM 结构要简单，而且这个结构后来也非常流行。 涉及材料","link":"/2020/04/13/%E7%90%86%E8%A7%A3lstm%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"BP神经网络","text":"一 前言 最近在看深度学习的东西，一开始看的吴恩达的UFLDL教程，有中文版就直接看了，后来发现有些地方总是不是很明确，又去看英文版，然后又找了些资料看，才发现，中文版的译者在翻译的时候会对省略的公式推导过程进行补充，但是补充的又是错的，难怪觉得有问题。 反向传播法其实是神经网络的基础了，但是很多人在学的时候总是会遇到一些问题，或者看到大篇的公式觉得好像很难就退缩了，其实不难，就是一个链式求导法则反复用。如果不想看公式，可以直接把数值带进去，实际的计算一下，体会一下这个过程之后再来推导公式，这样就会觉得很容易了。 二、简单的神经网络 说到神经网络，大家看到这个图应该不陌生： 这是典型的三层神经网络的基本构成，Layer L1是输入层，Layer L2是隐含层，Layer L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,…,xn},输出也是一堆数据{y1,y2,y3,…,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。 可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，我会专门再写一篇Auto-Encoder的文章来说明，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。三、前向传播 本文直接举一个例子，带入数值演示反向传播法的过程，公式的推导等到下次写Auto-Encoder的时候再写，其实也很简单，感兴趣的同学可以自己推导下试试。 3.1 初始化一个网络 假设，你有这样一个网络层： 第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。 现在对他们赋上初值，如下图： 其中，输入数据 i1=0.05，i2=0.10; 输出数据 o1=0.01,o2=0.99; 初始权重 w1=0.15,w2=0.20,w3=0.25,w4=0.30; w5=0.40,w6=0.45,w7=0.50,w8=0.55 目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。 3.2 Step 1 前向传播3.2.1.输入层—-&gt;隐含层： 计算神经元h1的输入加权和： 神经元h1的输出o1:(此处用到激活函数为sigmoid函数)： 同理，可计算出神经元h2的输出o2： 3.2.2.隐含层—-&gt;输出层： 计算输出层神经元o1和o2的值： 这样前向传播的过程就结束了，我们得到输出值为[0.75136079 , 0.772928465]，与实际值[0.01 , 0.99]相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。 四、反向传播4.1 计算总误差总误差：(square error) 但是有两个输出，所以分别计算o1和o2的误差，总误差为两者之和： 4.2 隐含层—-&gt;输出层的权值更新：以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则） 下面的图可以更直观的看清楚误差是怎样反向传播的： 现在我们来分别计算每个式子的值：计算 计算 （这一步实际上就是对sigmoid函数求导，比较简单，可以自己推导一下） 计算 最后三者相乘： 这样我们就计算出整体误差E(total)对w5的偏导值。 回过头来再看看上面的公式，我们发现： 为了表达方便，用来表示输出层的误差： 因此，整体误差E(total)对w5的偏导公式可以写成： 如果输出层误差计为负的话，也可以写成： 最后我们来更新w5的值（调整策略就是梯度下降）： （其中，是学习速率，这里我们取0.5）同理，可更新w6,w7,w8: 4.3 隐含层—-&gt;隐含层的权值更新： 方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)—-&gt;net(o1)—-&gt;w5,但是在隐含层之间的权值更新时，是out(h1)—-&gt;net(h1)—-&gt;w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。 计算 计算 同理，计算出： 两者相加得到总值： 再计算 再计算 最后，三者相乘： 为了简化公式，用sigma(h1)表示隐含层单元h1的误差： 最后，更新w1的权值（继续用梯度下降）： 同理，额可更新w2,w3,w4的权值： 这样误差反向传播法就完成了，最后我们再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为0.015912196,0.984065734,证明效果还是不错的。 实现代码请参见此文末尾：http://www.cnblogs.com/charlotte77/p/5629865.html","link":"/2020/05/05/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"思维模型","slug":"思维模型","link":"/tags/%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%9E%8B/"},{"name":"信息","slug":"信息","link":"/tags/%E4%BF%A1%E6%81%AF/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"人工神经网络","slug":"人工神经网络","link":"/tags/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"lstm","slug":"lstm","link":"/tags/lstm/"},{"name":"rnn","slug":"rnn","link":"/tags/rnn/"},{"name":"gru","slug":"gru","link":"/tags/gru/"}],"categories":[{"name":"知识","slug":"知识","link":"/categories/%E7%9F%A5%E8%AF%86/"},{"name":"计算机","slug":"知识/计算机","link":"/categories/%E7%9F%A5%E8%AF%86/%E8%AE%A1%E7%AE%97%E6%9C%BA/"},{"name":"感悟","slug":"知识/感悟","link":"/categories/%E7%9F%A5%E8%AF%86/%E6%84%9F%E6%82%9F/"},{"name":"计算机","slug":"计算机","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/"},{"name":"机器学习","slug":"计算机/机器学习","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"人工神经网络","slug":"机器学习/人工神经网络","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"信息论","slug":"机器学习/信息论","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BF%A1%E6%81%AF%E8%AE%BA/"},{"name":"BP神经网络","slug":"机器学习/BP神经网络","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]}